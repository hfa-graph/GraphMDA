# GraphMDA

This repository provides a reference implementation of **GraphMDA**, proposed in our paper  
*"GraphMDA: A Graph-Based Method for Malicious Traffic Detection with Minimal Data via Adversarial Domain Adaptation"* (under review).

---

## Graph_Builder

The `graph_builder` directory contains scripts for constructing, splitting, and preprocessing time-windowed NetFlow graphs used in GraphMDA.

* `subgraph_build.py`
  builds time-windowed flow graphs from raw NetFlow CSV files, where each window forms a graph with `same_src` and `same_dst` edges.

* `source_target_subgraph_split_same_environment.py`
  creates source/target splits under the same-environment leave-one-out strategy.

* `source_target_subgraph_split_across_environments.py`
  creates source/target splits across different network environments (e.g., CIC → BoT; BoT → ToN; ToN → CIC), and is also used to split real-world collected traffic into source and target domains (e.g., I → T).

* `train_test_split.py`
  splits each domain into train and test sets (75% / 25% by nodes) while preserving label distributions.

* `subgraph_preprocess.py`
  applies Target Encoding and feature standardization on each split, fitted only on the training data to prevent data leakage.

---

## Model_Train_Test

* `train_test_upload.py` 
  trains and evaluates the RGAT-based adversarial domain adaptation model on time-windowed NetFlow graphs from source and target domains.

  The script operates on the source–target graph splits generated by the `graph_builder` pipeline.

  Few-shot learning is simulated by randomly sampling a fixed **labeling budget** from `target/train` (e.g., 200, 1,000, and 5,000), with a **1:1 ratio** between benign (`label = 0`) and malicious (`label = 1`) samples whenever possible.  
  When one class is insufficient, the remaining budget is filled by selecting samples from the other class while preserving all original ground-truth labels.  
  Only these labeled target nodes are used to compute the **target-domain classification loss**.

  To mitigate cross-domain distribution shift, domain alignment is optimized through both **local and global discriminators**, encouraging the learned representations to be **domain-invariant**.

  During training, model checkpoints and per-budget evaluation metrics are automatically saved to the `checkpoints/` directory.

